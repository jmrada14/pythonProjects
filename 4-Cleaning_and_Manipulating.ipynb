{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Manipulating and Cleaning Data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You've learned a lot. Now it's time to practice applying some of that knowledge!\n\nReal-world data is messy. You will likely need to combine several data sources to get the data you actually want. The data from those sources will be incomplete. And it will likely not be formatted in exactly the way you want in order to perform your analysis. It's for these reasons that most data scientists will tell you that about 80 percent of any project is spent just getting the data into a form ready for analysis."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\n\n## Exploring `DataFrame` information\n\n> **Learning goal:** By the end of this subsection, you should be comfortable finding general information about the data stored in pandas DataFrames.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "When you start exploring data, your first step will likely be to load it into a pandas `DataFrame`. However, if the data set in your `DataFrame` has 60,000 rows and 400 columns, how do you even begin to get a sense of what you're working with? Fortunately, pandas provides some convenient tools to quickly look at overall information about a `DataFrame` in addition to the first few and last few rows.\n\nIn order to explore this functionality, we will import a Python library called scikit-learn library, which is a common library used in machine learning, and use an iconic dataset that every data scientist has seen hundreds of times: British biologist Ronald Fisher's *Iris* data set used in his 1936 paper \"The use of multiple measurements in taxonomic problems\":"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install sklearn",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\niris_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `DataFrame.info`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Share** - What does this tell us about our data?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "iris_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exploring a `DataFrame`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "iris_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:\n\nBy default, `DataFrame.head` returns the first five rows of a `DataFrame`. In the code cell below, can you figure out how to get it to show more?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Hint: Consult the documentation by using iris_df.head?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "iris_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Even just by looking at the metadata about the information in a DataFrame or the first and last few values in one, you can get an immediate idea about the size, shape, and content of the data you are dealing with."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Dealing with missing data\n\n> **Learning goal:** By the end of this subsection, you should know how to replace or remove null values from DataFrames.\n\n**None vs NaN**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `None`: non-float missing data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Most of the time datasets have missing values. How missing data is handled carries with it subtle tradeoffs that can affect your final analysis and real-world outcomes.\n\nPandas handles missing values in two ways. \n1. The first you've seen before in previous sections: `NaN`, or Not a Number. This is a actually a special value that is part of the IEEE floating-point specification and it is only used to indicate missing floating-point values.\n\n1. For missing values apart from floats, pandas uses the Python `None` object. \n\nBoth `None` and `NaN` carry restrictions that you need to be mindful of with regards to how they can be used."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nexample1 = np.array([2, None, 6, 8])\nexample1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Think, Pair, Share**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Recall how NumPy and pandas handles putting different data types into a series. \n\nHow will this affect operations on that series?\n\nWhat does this mean for aggregations?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example1.sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Key takeaway**: Addition (and other operations) between integers and `None` values is undefined, which can limit what you can do with datasets that contain them."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `NaN`: missing float values\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In contrast to `None`, NumPy and pandas support `NaN` for fast, vectorized operations and ufuncs. The bad news is that any arithmetic performed on `NaN` always results in `NaN`. For example:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.nan + 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.nan * 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Think, Pair, Share**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example2 = np.array([2, np.nan, 6, 8]) \nexample2.sum(), example2.min(), example2.max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What are the implications of this on analysis?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# What happens if you add np.nan and None together?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember: `NaN` is just for missing floating-point values; there is no `NaN` equivalent for integers, strings, or Booleans."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `NaN` and `None`: null values in pandas"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Even though `NaN` and `None` can behave somewhat differently, pandas is nevertheless built to handle them interchangeably. To see what we mean, consider a `Series` of integers:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "int_series = pd.Series([1, 2, 3], dtype=int)\nint_series",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now set an element of int_series equal to None.\n# How does that element show up in the Series?\n# What is the dtype of the Series?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the process of upcasting pandas will switch missing values between `None` and `NaN`. Because of this design feature, it can be helpful to think of `None` and `NaN` as two different flavors of \"null\" in pandas. Indeed, some of the core methods you will use to deal with missing values in pandas reflect this idea in their names:\n\n- `isnull()`: Generates a Boolean mask indicating missing values\n- `notnull()`: Opposite of `isnull()`\n- `dropna()`: Returns a filtered version of the data\n- `fillna()`: Returns a copy of the data with missing values filled or imputed\n\nThese are important methods to master and get comfortable with, so let's go over them each in some depth."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Detecting null values\n`isnull()` and `notnull()`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "empty_example = pd.Series([0, np.nan, '', None])\nempty_example",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Think, Pair, Share** - What will the outcome be?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "empty_example.isnull()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Try running empty_example[empty_example.notnull()].\n# Before you do so, what do you expect to see?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Key takeaway**: Both the `isnull()` and `notnull()` methods produce similar results when you use them in `DataFrame`s: they show the results and the index of those results, which will help you enormously as you wrestle with your data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dropping null values"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Beyond identifying missing values, pandas provides a convenient means to remove null values from `Series` and `DataFrame`s."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "empty_example = empty_example.dropna()\nempty_example",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that this should look like your output from `example3[example3.notnull()]`. The difference here is that, rather than just indexing on the masked values, `dropna` has removed those missing values from the `Series` `example3`.\n\nBecause `DataFrame`s have two dimensions, they afford more options for dropping data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4 = pd.DataFrame([[1,      np.nan, 7], \n                         [2,      5,      8], \n                         [np.nan, 6,      9]])\nexample4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note: check out the upcasting to accomodate the `NaN`s!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Think, Pair, Share**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4.dropna()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Drop from Columns**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is much less common, as usually columns represent features and rows represent individual observations - but here's how:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4.dropna(axis='columns')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What if we want to be a little less aggressive with our drop?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4.dropna?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "By default, `how='any'`. `how='all'` only drops rows or columns that contain all null values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4[3] = np.nan\nexample4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# How might you go about dropping just column 3?\n# Hint: remember that you will need to supply both the axis parameter and the how parameter.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `thresh` parameter gives you finer-grained control: you set the number of *non-null* values that a row or column needs to have in order to be kept.\n\n**Think, Pair, Share**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4.dropna(axis='rows', thresh=3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Filling null values"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Sometimes it makes more sense to fill null values with valid ones rather than drop them. You could use `isnull` to do this in place, but that can be laborious, particularly if you have a lot of values to fill. \n\n`fillna` returns a copy of the `Series` or `DataFrame` with the missing values replaced with one of your choosing."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example5 = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\nexample5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example5.fillna(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# What happens if you try to fill null values with a string, like ''?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What are some examples of situations where it might be better to fill instead of drop? And vice versa?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Forward-fill**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example5.fillna(method='ffill')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Back-fill**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example5.fillna(method='bfill')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "**Specify Axis**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4.fillna(method='ffill', axis='columns')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that when a previous value is not available for forward-filling, the null value remains."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# What output does example4.fillna(method='bfill', axis=1) produce?\n# What about example4.fillna(method='ffill') or example4.fillna(method='bfill')?\n# Can you think of a longer code snippet to write that can fill all of the null values in example4?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Fill with Logical Data**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example4.fillna(example4.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that column 3 is still valueless: the default direction is to fill values row-wise."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\n> **Takeaway:** There are multiple ways to deal with missing values in your datasets. The specific strategy you use (removing them, replacing them, or even how you replace them) should be dictated by the particulars of that data. You will develop a better sense of how to deal with missing values the more you handle and interact with datasets."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Removing duplicate data\n\n> **Learning goal:** By the end of this subsection, you should be comfortable identifying and removing duplicate values from DataFrames.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, pandas provides an easy means of detecting and removing duplicate entries."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Identifying duplicates: `duplicated`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`duplicated` returns a Boolean mask indicating whether an entry in a `DataFrame` is a duplicate of an ealier one."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example6 = pd.DataFrame({'letters': ['A','B'] * 2 + ['B'],\n                         'numbers': [1, 2, 1, 3, 3]})\nexample6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example6.duplicated()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dropping duplicates: `drop_duplicates`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`drop_duplicates` returns a copy of the data for which all of the `duplicated` values are `False`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example6.drop_duplicates()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Both `duplicated` and `drop_duplicates` default to consider all columns but you can specify that they examine only a subset of columns in your `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "example6.drop_duplicates(['letters'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Removing duplicate data is an essential part of almost every data-science project. Duplicate data can change the results of your analyses and give you spurious results!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Combining datasets: merge and join\n\n> **Learning goal:** By the end of this subsection, you should have a general knowledge of the various ways to combine `DataFrame`s."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Your most interesting analyses will often come from data melded together from more than one source. Because of this, pandas provides several methods of merging and joining datasets to make it easier:\n - **`pandas.merge`** connects rows in `DataFrame`s based on one or more keys.\n - **`pandas.concat`** concatenates or “stacks” together objects along an axis.\n - The **`combine_first`** instance method enables you to splice together overlapping data to fill in missing values in one object with values from another.\n\nLet's examine merging data first, because it will be the most familiar to course attendees who are already familiar with SQL or other relational databases."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Categories of joins\n\n`merge` carries out several types of joins: *one-to-one*, *many-to-one*, and *many-to-many*."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### One-to-one joins\n\nConsider combining two `DataFrame`s that contain different information on the same employees in a company:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = pd.DataFrame({'employee': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'group': ['Accounting', 'Marketing', 'Marketing', 'HR']})\ndf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2 = pd.DataFrame({'employee': ['Mary', 'Stu', 'Gary', 'Sue'],\n                    'hire_date': [2008, 2012, 2017, 2018]})\ndf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Combine this information into a single `DataFrame` using the `merge` function:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Think, Pair, Share**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df3 = pd.merge(df1, df2)\ndf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that pandas discarded the original indices of `df1` and `df2`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Many-to-one joins"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A many-to-one join is like a one-to-one join except that one of the two key columns contains duplicate entries. The `DataFrame` resulting from such a join will preserve those duplicate entries as appropriate:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df4 = pd.DataFrame({'group': ['Accounting', 'Marketing', 'HR'],\n                    'supervisor': ['Carlos', 'Giada', 'Stephanie']})\ndf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Share**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df3, df4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The resulting `DataFrame` has an additional column for `supervisor`; that column has an extra occurence of 'Giada' that did not occur in `df4` because more than one employee in the merged `DataFrame` works in the 'Marketing' group.\n\nNote that we didn’t specify which column to join on. When you don't specify that information, `merge` uses the overlapping column names as the keys. However, that can be ambiguous; several columns might meet that condition. For that reason, it is a good practice to explicitly specify on which key to join. You can do this with the `on` parameter:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Specify Key**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df3, df4, on='group')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Many-to-many joins"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What happens if the key columns in both of the `DataFrame`s you are joining contain duplicates? That gives you a many-to-many join:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n                              'Marketing', 'Marketing', 'HR', 'HR'],\n                    'core_skills': ['math', 'spreadsheets', 'writing', 'communication',\n                               'spreadsheets', 'organization']})\ndf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, on='group')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `left_on` and `right_on` keywords"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What if you need to merge two datasets with no shared column names? \n\nFor example, what if you are using a dataset in which the employee name is labeled as 'name' rather than 'employee'? In such cases, you will need to use the `left_on` and `right_on` keywords in order to specify the column names on which to join:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df6 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'salary': [70000, 80000, 120000, 90000]})\ndf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for_exercise = pd.merge(df1, df6, left_on=\"employee\", right_on=\"name\")\nfor_exercise",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Using the documentation, can you figure out how to use .drop() to get rid of the 'name' column?\n# Hint: You will need to supply two parameters to .drop()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `left_index` and `right_index` keywords"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Sometimes it can be more advantageous to merge on an index rather than on a column. The `left_index` and `right_index` keywords make it possible to join by index."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1a = df1.set_index('employee')\ndf1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2a = df2.set_index('employee')\ndf2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df1a, df2a, left_index=True, right_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# What happens if you specify only left_index or right_index?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**`join` for `DataFrame`s**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also use the `join` method for `DataFrame`s, which produces the same effect but merges on indices by default:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1a.join(df2a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Mix and Match**: `left_index`/`right_index` with `right_on`/`left_on`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df1a, df6, left_index=True, right_on='name')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Set arithmetic for joins"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's return to many-to-many joins for a moment. A consideration that is unique to them is the *arithmetic* of the join, specifically the set arithmetic we use for the join."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df5 = pd.DataFrame({'group': ['Engineering', 'Marketing', 'Sales'],\n                    'core_skills': ['math', 'writing', 'communication']})\ndf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, on='group')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Whoa, we have only two entries in the result. \n\nThis is because we merged on `group` and 'Marketing' was the only entry that appeared in the `group` column of both `DataFrame`s.\n\nIn effect, what we have gotten is the *intersection* of both `DataFrame`s. This is know as the inner join in the database world and it is the default setting for `merge` although we can certainly specify it:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**`intersection` for merge**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, on='group', how='inner')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The complement of the inner join is the outer join, which returns the *union* of the two `DataFrame`s."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The keyword for perfoming an outer join is how='outer'. How would you perform it?\n# What do you expect the output of an outer join of df1 and df5 to be?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Inner and outer joins are not your only options. A *left join* returns all of the rows in the first (left-side) `DataFrame` supplied to `merge` along with rows from the other `DataFrame` that match up with the left-side key values (and `NaNs` rows with respective values):"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Share**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now run the right merge between df1 and df5.\n# What do you expect to see?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `suffixes` keyword: dealing with conflicting column names"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because you can join datasets, you will eventually join two with conflicting column names. Let's look at another example to see what we mean:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df7 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'rank': [1, 2, 3, 4]})\ndf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df8 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'rank': [3, 1, 4, 2]})\ndf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df7, df8, on='name')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `merge` function automatically appends the suffix `_x` or `_y` to the conflicting column names in order to make them unique. \n\nIn cases where it is best to control your column names, you can specify a custom suffix for `merge` to append through the `suffixes` keyword:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Using `_` to merge same column names**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.merge(df7, df8, on='name', suffixes=['_left', '_right'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenation in NumPy"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Concatenation in pandas is built off of the concatenation functionality for NumPy arrays. \n\nHere is a quick review of what NumPy concatenation looks like:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**One-dimensional arrays**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [1, 2, 3]\ny = [4, 5, 6]\nz = [7, 8, 9]\nnp.concatenate([x, y, z])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Two-dimensional arrays**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [[1, 2],\n     [3, 4]]\nnp.concatenate([x, x], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the `axis=1` parameter makes the concatenation occur along columns rather than rows. Concatenation in pandas looks similar to this."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenation in pandas"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pandas has a function, `pd.concat()` that can be used for a simple concatenation of `Series` or `DataFrame` objects in similar manner to `np.concatenate()` with ndarrays."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Series**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ser1 = pd.Series(['a', 'b', 'c'], index=[1, 2, 3])\nser2 = pd.Series(['d', 'e', 'f'], index=[4, 5, 6])\npd.concat([ser1, ser2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**DataFrames**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df9 = pd.DataFrame({'A': ['a', 'c'],\n                    'B': ['b', 'd']})\ndf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.concat([df9, df9])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that `pd.concat` has preserved the indexing even though that means that it has been duplicated. You can have the results re-indexed (and avoid potential confusion down the road) like so:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Re-indexing**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.concat([df9, df9], ignore_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "By default, `pd.concat` concatenates row-wise within the `DataFrame` (that is, `axis=0` by default). You can specify the axis along which to concatenate:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.concat([df9, df9], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that while pandas will display this without error, you will get an error message if you try to assign this result as a new `DataFrame`. Column names in `DataFrame`s must be unique."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenation with joins"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Just as you did with merge above, you can use inner and outer joins when concatenating `DataFrame`s with different sets of column names."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df10 = pd.DataFrame({'A': ['a', 'd'],\n                     'B': ['b', 'e'],\n                     'C': ['c', 'f']})\ndf10",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df11 = pd.DataFrame({'B': ['u', 'x'],\n                     'C': ['v', 'y'],\n                     'D': ['w', 'z']})\ndf11",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.concat([df10, df11])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we saw earlier, the default join for this is an outer join and entries for which no data is available are filled with `NaN` values. You can also do an inner join:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.concat([df10, df11], join='inner')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another option is to directly specify the index of the remaininig colums using the `join_axes` argument, which takes a list of index objects. Here, we will specify that the returned columns should be the same as those of the first input (`df10`):"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.concat([df10, df11], join_axes=[df10.columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `append()`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because direct array concatenation is so common, ``Series`` and ``DataFrame`` objects have an ``append`` method that can accomplish the same thing in fewer keystrokes. For example, rather than calling ``pd.concat([df9, df9])``, you can simply call ``df9.append(df9)``:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df9.append(df9)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Important point**: Unlike the `append()` and `extend()` methods of Python lists, the `append()` method in pandas does not modify the original object. It instead creates a new object with the combined data.\n\n> **Takeaway:** A large part of the value you can provide as a data scientist comes from connecting multiple, often disparate datasets to find new insights. Learning how to join and merge data is thus an essential part of your skill set."
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}